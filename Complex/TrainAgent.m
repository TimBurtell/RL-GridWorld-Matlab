
%% Create the Environment for the actor. This replaces the map load/ and timestepping behavior 
% https://www.mathworks.com/help/reinforcement-learning/ref/rl.env.rlfunctionenv.html
[observationInfo, actionInfo, stepFunction, resetFunction] = EnvironmentSetup();
env = rlFunctionEnv(observationInfo, actionInfo, stepFunction, resetFunction);


%% Create the Actor. This replaces the robot() function
% Create network 
inputDimensions = observationInfo.Dimension;
numActions = 8;
actorNetwork =                                                                           ...
    [                                                                                    ...
        imageInputLayer(inputDimensions, 'Normalization', 'none', 'Name', 'state'),       ...
        convolution2dLayer(3, 50),                                                       ...
        reluLayer, ...
        fullyConnectedLayer(120, 'Name', 'H1'),                                          ... 
        reluLayer,                                                                       ...
        fullyConnectedLayer(120, 'Name', 'H2'),                                           ...  
        reluLayer,                                                                       ...
        fullyConnectedLayer(8, 'Name', 'H3'),                                            ... 
        reluLayer,                                                                       ...
        softmaxLayer('Name', 'action')
    ];         
actorOptions = rlRepresentationOptions('LearnRate', 5e-3,        ...
                                       'GradientThreshold', 1,   ...
                                       'UseDevice', "gpu");

actor = rlRepresentation(actorNetwork, actorOptions, ...
        'Observation', {'state'}, observationInfo,           ... 
        'Action', {'action'}, actionInfo);


%% Create agent using the actor
agentOptions = rlPGAgentOptions(  ...
        'UseBaseline', false,     ...
        'EntropyLossWeight', .90,  ...
        'DiscountFactor', 0.90);
agent = rlPGAgent(actor, agentOptions);

%% Train the network 
trainOptions = rlTrainingOptions(              ...
            'MaxEpisodes', 10000000,                   ...
            'MaxStepsPerEpisode', 100,               ...
            'Verbose', 0,                            ...
            'SaveAgentDirectory', pwd  + "agents");


trainResults = train(agent, env, trainOptions);
save agent
%rubble2(3);
